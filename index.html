<!DOCTYPE html>
<!--
	Hyperspace by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
  <head>
    <title>Hyperspace by HTML5 UP</title>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, user-scalable=no"
    />
    <link rel="stylesheet" href="assets/css/main.css" />
    <noscript
      ><link rel="stylesheet" href="assets/css/noscript.css"
    /></noscript>
  </head>
  <body class="is-preload">
    <!-- Sidebar -->
    <section id="sidebar">
      <div class="inner">
        <nav>
          <ul>
            <li><a href="#intro">Welcome</a></li>
            <li><a href="#one">Unit 1</a></li>
            <li><a href="#two">Unit 2</a></li>
            <li><a href="#three">Unit 3</a></li>
            <li><a href="#four">Unit 4</a></li>
            <li><a href="#five">Unit 5</a></li>
            <li><a href="#six">Unit 6</a></li>
            <li><a href="#seven">Unit 7</a></li>
            <li><a href="#eight">Unit 8</a></li>
            <li><a href="#nine">Unit 9</a></li>
            <li><a href="#ten">Unit 10</a></li>
            <li><a href="#eleven">Unit 11</a></li>
            <li><a href="#twelve">Unit 12</a></li>
          </ul>
        </nav>
      </div>
    </section>

    <!-- Wrapper -->
    <div id="wrapper">
      <!-- Intro -->
      <section id="intro" class="wrapper style1 fullscreen fade-up">
        <div class="inner">
          <h1>e-Portfolio</h1>
          <p>Master’s in Data Science | University of Essex Online</p>
          <p>
            Software Engineer with experience in data science, automation, and
            front-end development. I thrive in dynamic environments and focus on
            building meaningful, user-centred solutions. Open to continuous
            learning and change, I stay current with new technologies and use a
            collaborative mindset and constructive feedback to improve both
            individual and team outcomes.
          </p>
          <a
            href="modules/deciphering-big-data/artefacts/Anonymous CV.pdf"
            target="_blank"
            style="font-size: 50px"
            >Full CV</a
          >
        </div>
      </section>

      <!-- One -->
      <section id="one" class="wrapper style2 fade-up">
        <div class="inner">
          <h2>
            Unit 1: Introduction to Big Data Technologies and Data Management
          </h2>

          <h2 style="margin-bottom: 5px">Reflection</h2>
          <p>
            <strong>What:</strong> During Unit 1, the tutor has taught us how to
            identify and evaluate various data formats (structure,
            semi-structured, and unstructured), and also how to understand the
            impact of data storage and security on system performance.
            <br /><br />
            <strong>So what:</strong> This Unit has aided me in connecting
            theoretical knowledge of big data with practical implementation,
            particularly when it comes to using Python for cleaning and
            transformation processes of data.<br /><br />
            <strong>What next:</strong> In future units, I aim to apply these
            concepts to real-world data sets and strengthen my data validation
            and visualization skills.
          </p>

          <h2>Artefact: Big Data Overview in Swiss Cities</h2>
          <p>
            This Jupyter Notebook explores a simple dataset of Swiss cities,
            comparing population size, data centre presence, and internet
            speeds.
          </p>

          <iframe
            src="https://nbviewer.org/github/uoe-anonymous-account/eportfolio/blob/main/modules/deciphering-big-data/artefacts/bigdata-intro-unit1.ipynb.ipynb"
            width="100%"
            height="800px"
          >
          </iframe>
          <i
            ><p>
              If nbviewer doesn't work, please access the file at the Github:
              <a
                href="https://github.com/uoe-anonymous-account/eportfolio/blob/main/modules/deciphering-big-data/artefacts/bigdata-intro-unit1.ipynb.ipynb"
                target="_blank"
              >
                Big Data - Introduction
              </a>
            </p></i
          >

          <h3>Documents and notes from the Seminar of Unit 1:</h3>
          <ul>
            <li>
              <a
                href="modules/deciphering-big-data/artefacts/Seminar 1  - Introduction - Summary.txt"
                target="_blank"
                >Seminar Summary (TXT)</a
              >
            </li>
            <li>
              <a
                href="modules/deciphering-big-data/artefacts/Seminar 1 - Introduction - PPTX"
                target="_blank"
                >Seminar Slides (PPTX)</a
              >
            </li>
            <li>
              <a
                href="modules/deciphering-big-data/artefacts/Seminar 1 - Introduction - Transcript.vtt"
                target="_blank"
                >Seminar Transcript (VTT)</a
              >
            </li>
          </ul>

          <h2>References</h2>
          <p>
            McKinney, W. (2022)
            <em
              >Python for Data Analysis: Data Wrangling with Pandas, NumPy, and
              Jupyter.</em
            >
            3rd edn. Sebastopol, California: O’Reilly.<br />
            Sardar, T. H. and Pandey, B.K. (2024)
            <em>Big Data Computing.</em> CRC Press.<br />
            Huxley et al. (2020) <em>Data Cleaning.</em> Sage Foundation.
          </p>
        </div>
      </section>

      <!-- Two -->
      <section id="two" class="wrapper style3 fade-up">
        <div class="inner">
          <h2>Unit 2: Data Files and Types</h2>
          <h2 style="margin-bottom: 5px">Reflection</h2>
          <p>
            <strong>What:</strong> This unit examined how data structures affect
            storage and access, comparing structured, semi-structured, and
            unstructured formats using the EMC (2015) pyramid model and Python
            data types.<br /><br />
            <strong>So what:</strong> I previously saw data formats as technical
            details, but peer perspectives revealed how design choices embed
            ethical trade-offs. Realising my bias toward performance over
            privacy made me reconsider how decentralisation and accountability
            shape pipeline design at work.<br /><br />
            <strong>What next:</strong> I plan to test small edge deployments
            with open datasets to study how preprocessing impacts latency, cost,
            and GDPR compliance, documenting results to improve collaboration.
          </p>
          <h2>Artefacts</h2>
          <ul>
            <li>
              <h3>Reflection on IoT Collaborative Discussion</h3>
              <p>
                <strong>What:</strong> I reviewed peer posts on IoT and fog
                computing, including Sonya Jackson’s analysis of latency
                reduction through decentralised processing.
              </p>
              <p>
                <strong>So what:</strong> This deepened my view of edge AI
                ethics and data ownership and turned my missed participation
                into a lesson in self-accountability.
              </p>
              <p>
                <strong>What next:</strong> I will explore how local
                preprocessing can reduce cloud dependency while strengthening
                GDPR compliance.
              </p>
              <p>
                As I was unable to participate in the "Collaborative Discussion
                1 - The Data Collection Process", I emailed the tutor and
                requested advice, to which I was encouraged to further reflect
                on observations and teachings. This exchange can be seen below.
              </p>
              <img
                width="100%"
                src="modules/deciphering-big-data/artefacts/email-with-tutor.png"
                alt=""
              />
            </li>
          </ul>

          <h2>References</h2>
          <p>
            Kazil, J. and Jarmul, K. (2016)
            <em>Data Wrangling with Python.</em> O’Reilly Media.<br />
            Services, EMC (2015)
            <em>Data Science and Big Data Analytics.</em> Wiley Professional
            Development.<br />
            Ohlhorst, F. J. (2012)
            <em>Big Data Analytics: Turning Big Data into Big Money.</em> Wiley.
          </p>
        </div>
      </section>

      <!-- Three -->
      <section id="three" class="wrapper style1 fade-up">
        <div class="inner">
          <h2>Unit 3: Data Collection and Storage</h2>

          <h2 style="margin-bottom: 5px">Reflection</h2>

          <p>
            <strong>What:</strong> This unit focused on automated data
            collection and validation using web scraping, addressing integrity,
            reliability, and format challenges. I applied BeautifulSoup and
            Requests to extract structured data into JSON and XML formats.
          </p>

          <p>
            <strong>So what:</strong> Building the scraper clarified how APIs
            and HTML parsing differ in control and efficiency while underscoring
            the importance of ethical and GDPR-compliant practices. Debugging
            improved my understanding of HTML hierarchies and response handling.
          </p>

          <p>
            <strong>What next:</strong> I plan to extend the project by scraping
            multi-page datasets and consolidating results into a database to
            test large-scale pipeline automation and compliance checks.
          </p>

          <h2>Artefact: Web Scraping Demonstration</h2>
          <p>
            This artefact demonstrates a Python-based scraper that collects
            headings and introductory text from a live Wikipedia article and
            exports the data to both JSON and XML formats.
          </p>

          <iframe
            src="https://nbviewer.org/github/uoe-anonymous-account/eportfolio/blob/main/modules/deciphering-big-data/artefacts/web-scraping-unit3.ipynb"
            width="100%"
            height="800px"
          >
          </iframe>
          <i
            ><p>
              If nbviewer doesn't work, please access the file at the Github:
              <a
                href="https://github.com/uoe-anonymous-account/eportfolio/blob/main/modules/deciphering-big-data/artefacts/web-scraping-unit3.ipynb"
                target="_blank"
              >
                Web Scraping
              </a>
            </p></i
          >
          <br />
          <div class="artefact-images">
            <figure>
              <p></p>
              <img
                src="modules/deciphering-big-data/artefacts/unit3_json.png"
                alt="Preview of the generated JSON file"
                width="90%"
              />
              <figcaption>
                Figure 2 – JSON output showing structured scraped data.
              </figcaption>
            </figure>

            <figure>
              <p></p>
              <img
                src="modules/deciphering-big-data/artefacts/unit3_xml.png"
                alt="Browser view of the XML output"
                width="90%"
              />
              <figcaption>
                Figure 3 – XML file displaying article summary and section
                headings.
              </figcaption>
            </figure>
          </div>
          <br />
          <h2>References</h2>
          <ul>
            <li>
              Kazil, J. and Jawal, K. (2016)
              <em>Data Wrangling with Python</em>. O’Reilly Media.
            </li>
            <li>
              Ohlhorst, F. J. (2012)
              <em>Big Data Analytics: Turning Big Data into Big Money</em>.
              Wiley.
            </li>
            <li>
              Beautiful Soup Documentation (2025) Available at:
              <a
                href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/"
                target="_blank"
                >https://www.crummy.com/software/BeautifulSoup/bs4/doc/</a
              >
            </li>
          </ul>
        </div>
      </section>

      <!-- Four-->
      <section id="four" class="wrapper style2 fade-up">
        <div class="inner">
          <h2>Unit 4: Data Cleaning and Transformation</h2>

          <h2 style="margin-bottom: 5px">Reflection</h2>

          <p>
            <strong>What:</strong> This unit covered data cleaning and
            transformation in Python, focusing on pipeline design, efficiency,
            and automation. I practised standardising data, handling missing
            values, and applying clean-code conventions such as PEP 8 and
            modular design.
          </p>

          <p>
            <strong>So what:</strong> The quiz reinforced the value of writing
            reusable, well-documented code and showed how structure and version
            control improve long-term maintainability. Achieving a perfect score
            revealed how much logic I had internalised, but also reminded me
            that real data work requires adaptability beyond syntax accuracy.
          </p>

          <p>
            <strong>What next:</strong> I plan to enhance my cleaning workflows
            using pandas and NumPy with automated validation checks and regular
            code reviews to ensure reliability and continuous improvement.
          </p>

          <h2>Artefact: Data Management Pipeline Quiz</h2>
          <p>
            Formative Python quiz demonstrating proficiency in data cleaning,
            syntax, and automation principles. It confirmed technical
            understanding while prompting reflection on flexibility and
            problem-solving in less controlled contexts.
          </p>

          <div class="artefact-images">
            <figure>
              <img
                src="modules/deciphering-big-data/artefacts/unit4-quiz-question1.png"
                alt="Screenshot of quiz question 1 result"
                width="90%"
              />
              <figcaption>
                Figure 1 – Python concept matching demonstrating comprehension
                of syntax and logical flow in cleaning operations.
              </figcaption>
            </figure>

            <br /><br />
            <figure>
              <img
                src="modules/deciphering-big-data/artefacts/unit4-quiz-question2.png"
                alt="Screenshot of quiz question 2 result"
                width="90%"
              />
              <figcaption>
                Figure 2 – Correct mapping of best practices in repository
                structure, documentation, and test design.
              </figcaption>
            </figure>
          </div>

          <br />
          <h2>References</h2>
          <ul>
            <li>
              McKinney, W. (2022)
              <em
                >Python for Data Analysis: Data Wrangling with Pandas, NumPy,
                and Jupyter</em
              >. 3rd edn. O’Reilly Media.
            </li>
            <li>
              Huxley et al. (2020) <em>Data Cleaning</em>. Sage Foundation.
            </li>
            <li>
              PEP 8 – Style Guide for Python Code (2025). Available at:
              <a href="https://peps.python.org/pep-0008/" target="_blank"
                >https://peps.python.org/pep-0008/</a
              >
            </li>
          </ul>
        </div>
      </section>

      <!-- Five-->
      <section id="five" class="wrapper style3 fade-up">
        <div class="inner">
          <h2>Unit 5: Data Cleaning, Automation and Validation</h2>

          <h2 style="margin-bottom: 5px">Reflection</h2>

          <p>
            <strong>What:</strong> This unit focused on automation and
            validation as key elements of reliable data workflows. Through
            Seminar 3 on Data Investigations, I learned how unverified sources
            can introduce bias, linking directly to my earlier web-scraping work
            and the need for transparent logging
          </p>

          <p>
            <strong>So what:</strong> I realised that automation can create
            misplaced confidence if data quality is weak. Even accurate scripts
            fail when inputs lack validation, highlighting that efficiency must
            coexist with accountability. This reframed data cleaning as both a
            technical and ethical process.
          </p>

          <p>
            <strong>What next:</strong> I plan to embed verification steps and
            logging into future Python workflows to track data provenance and
            strengthen governance practices.
          </p>

          <h2>Artefact: Seminar 3 – Data Investigations</h2>

          <ul>
            <p style="margin-bottom: 5px">
              Seminar materials exploring data quality, automation, and ethics
              in modern workflows, reinforcing the need for transparency and
              reproducibility in data processes.
            </p>
            <li>
              <a
                href="modules/deciphering-big-data/artefacts/Seminar%203%20-%20Data%20Investigations%20-%20Summary.txt"
                target="_blank"
                >Seminar Summary (TXT)</a
              >
            </li>
            <li>
              <a
                href="modules/deciphering-big-data/artefacts/Seminar 3 - Data Investigations - PPTX.pptx"
                target="_blank"
                >Seminar Slides (PPTX)</a
              >
            </li>
            <li>
              <a
                href="modules/deciphering-big-data/artefacts/Seminar%203%20-%20Data%20Investigations%20-%20Transcript.vtt"
                target="_blank"
                >Seminar Transcript (VTT)</a
              >
            </li>
          </ul>

          <h2>References</h2>
          <ul>
            <li>
              Raschka, S. (2021)
              <em>Machine Learning and Data Cleaning in Practice</em>. Packt
              Publishing.
            </li>
            <li>
              Kazil, J. and Jarmul, K. (2016)
              <em>Data Wrangling with Python</em>. O’Reilly Media.
            </li>
            <li>
              University of Essex Online (2025) Seminar 3:
              <em>Data Investigations</em> [Lecture material].
            </li>
          </ul>
        </div>
      </section>

      <!-- Six-->
      <section id="six" class="wrapper style1 fade-up">
        <div class="inner">
          <h2>Unit 6: Group Project – Clinical Research Database Design</h2>

          <h2 style="margin-bottom: 5px">Reflection</h2>
          <p>
            <strong>What:</strong> This unit centred on collaborative database
            design for clinical research compliance under GDPR and GCP. I joined
            the group late and restructured an incomplete draft, improving
            coherence, critical depth, and visuals such as the ERD and Data
            Management Pipeline.
          </p>

          <p>
            <strong>So what:</strong> Revising an existing project highlighted
            the need for synthesis between technical, ethical, and regulatory
            elements. It also revealed the challenges of asynchronous teamwork
            and how constructive leadership can emerge through action rather
            than authority. The final report demonstrated clearer analysis and
            design thinking.
          </p>

          <p>
            <strong>What next:</strong> I plan to apply these lessons by
            defining roles earlier in future collaborations and developing the
            ERD and pipeline further into a PostgreSQL prototype with automated
            validation.
          </p>

          <h2>Artefacts</h2>
          <ul>
            <li>
              <a
                href="modules/deciphering-big-data/artefacts/Data%20Management%20Pipeline%20and%20Database%20Design%20for%20Clinical%20Research%20Compliance.docx"
                target="_blank"
                >Final Report – Data Management Pipeline and Database Design for
                Clinical Research Compliance</a
              >
            </li>
            <li>
              <a
                href="modules/deciphering-big-data/artefacts/Initial%20group%20draft.docx"
                target="_blank"
                >Initial Group Draft</a
              >
            </li>
            <li>
              <a
                href="modules/deciphering-big-data/artefacts/Peer%20Evaluation.pdf"
                target="_blank"
                >Peer Evaluation</a
              >
            </li>
          </ul>

          <h2>References</h2>
          <ul>
            <li>
              European Union (2016)
              <em>General Data Protection Regulation (GDPR)</em>.
            </li>
            <li>
              Sarkar, T. & Roychowdhury, S. (2019)
              <em>Data Wrangling with Python.</em> Packt Publishing.
            </li>
            <li>
              European Medicines Agency (2023)
              <em
                >Guideline on Computerised Systems and Electronic Data in
                Clinical Trials.</em
              >
            </li>
            <li>
              Williams, G. (2025) <em>Deciphering Big Data</em> [Module
              materials]. University of Essex Online.
            </li>
          </ul>
        </div>
      </section>

      <!-- Seven-->
      <section id="seven" class="wrapper style2 fade-up">
        <div class="inner">
          <h2>Unit 7: Normalisation and Relational Database Construction</h2>

          <h2 style="margin-bottom: 5px">Reflection</h2>
          <p>
            <strong>What:</strong> This unit applied relational database
            principles through a normalisation task and seminar. I decomposed a
            denormalised dataset into 1NF, 2NF, and 3NF with primary and foreign
            keys, reinforcing how structured design prevents anomalies and
            improves data integrity.
          </p>

          <p>
            <strong>So what:</strong> The exercise showed how redundancy
            undermines consistency and how normalisation supports scalability
            and analytics. I also learned why partial denormalisation is
            sometimes used for performance, linking theory directly to SQL
            commands such as <code>CREATE TABLE</code> and <code>JOIN</code>.
          </p>

          <p>
            <strong>What next:</strong> I plan to use these modelling techniques
            in Python and PostgreSQL projects, maintaining normalisation through
            data pipelines and employing visual schema diagrams to identify
            dependencies early in design.
          </p>

          <h2>Artefact - Data Normalisation Task</h2>
          <iframe
            src="https://nbviewer.org/github/uoe-anonymous-account/eportfolio/blob/main/modules/deciphering-big-data/artefacts/data-normalisation-unit7.ipynb"
            width="100%"
            height="800"
            frameborder="0"
            loading="lazy"
          >
          </iframe>
          <i
            ><p>
              If nbviewer doesn't work, please access the file at the Github:
              <a
                href="https://github.com/uoe-anonymous-account/eportfolio/blob/main/modules/deciphering-big-data/artefacts/data-normalisation-unit7.ipynb"
                target="_blank"
              >
                Data Normalisation
              </a>
            </p></i
          >
          <div class="downloads">
            <h3>View normalized tables output:</h3>
            <ul>
              <li>
                <a
                  href="https://github.com/uoe-anonymous-account/eportfolio/blob/main/modules/deciphering-big-data/artefacts/courses.csv"
                  target="_blank"
                  >Courses</a
                >
              </li>
              <li>
                <a
                  href="https://github.com/uoe-anonymous-account/eportfolio/blob/main/modules/deciphering-big-data/artefacts/enrollments.csv"
                  target="_blank"
                  >Enrollments</a
                >
              </li>
              <li>
                <a
                  href="https://github.com/uoe-anonymous-account/eportfolio/blob/main/modules/deciphering-big-data/artefacts/students.csv"
                  target="_blank"
                  >Students</a
                >
              </li>
              <li>
                <a
                  href="https://github.com/uoe-anonymous-account/eportfolio/blob/main/modules/deciphering-big-data/artefacts/teachers.csv"
                  target="_blank"
                  >Teachers</a
                >
              </li>
            </ul>
          </div>

          <h2>Artefact - Seminar 4 - SQL Normalisation</h2>
          <ul>
            <li>
              <a
                href="modules/deciphering-big-data/artefacts/Seminar%204%20-%20SQL%20Normalization%20-%20Summary.txt"
                target="_blank"
                >Seminar 4 – SQL Normalisation Summary</a
              >
            </li>
            <li>
              <a
                href="modules/deciphering-big-data/artefacts/Seminar%204%20-%20SQL%20Normalization%20-%20PPTX.pptx"
                target="_blank"
                >Seminar Slides (PPTX)</a
              >
            </li>
            <li>
              <a
                href="modules/deciphering-big-data/artefacts/Seminar%204%20-%20SQL%20Normalization%20-%20Transcript.vtt"
                target="_blank"
                >Seminar Transcript (VTT)</a
              >
            </li>
          </ul>

          <h2>References</h2>
          <ul>
            <li>
              Kazil, J. and Jarmul, K. (2016)
              <em>Data Wrangling with Python</em>. O’Reilly Media.
            </li>
            <li>
              Williams, G. (2025)
              <em>Seminar 4: SQL and Normalisation</em> [Lecture material].
              University of Essex Online.
            </li>
            <li>
              Codd, E. F. (1970) ‘A relational model of data for large shared
              data banks’, <em>Communications of the ACM</em>, 13(6), pp.
              377–387.
            </li>
          </ul>
        </div>
      </section>

      <!-- Eigth -->
      <section id="eight" class="wrapper style3 fade-up">
        <div class="inner">
          <h2>Unit 8: Compliance and Regulatory Framework for Managing Data</h2>

          <h2 style="margin-bottom: 5px">Reflection</h2>
          <p>
            <strong>What:</strong> This unit examined data protection frameworks
            such as the EU and UK GDPR, focusing on how compliance laws ensure
            security and accountability. The collaborative discussion compared
            implementation across regions, clarifying both shared ethics and
            post-Brexit legal nuances.
          </p>

          <p>
            <strong>So what:</strong> Peers’ insights revealed contrasting
            organisational attitudes toward compliance. Some viewed
            certification schemes like ISO 27001 as safeguards, while others
            highlighted the risk of treating compliance as a checklist. I
            realised that true compliance relies on continuous review and
            cultural awareness rather than documentation alone. This shifted my
            understanding of data protection from obligation to design
            principle.
          </p>

          <p>
            <strong>What next:</strong> I plan to integrate privacy and
            governance into system design, treating frameworks like GDPR as
            enablers of trust. I aim to strengthen automated audit trails and
            consent management across future projects.
          </p>

          <h2>Artefact - Collaborative Discussion 2 Reflection</h2>
          <p>
            <strong>Discussion Topic:</strong> Compare the rules of the GDPR, in
            particular with relation to the securing of personal data rule, with
            either similar compliance laws within your country of residence or
            with the ICO in the UK. The ICO refers to this rule as
            <em>Security</em> and asks organisations to ensure that personal
            data is processed in a manner that guarantees appropriate security
            and prevents unauthorised access (<a
              href="https://ico.org.uk/"
              target="_blank"
              >ICO.org.uk</a
            >).
          </p>
          <p>
            <strong>Learning Outcomes:</strong> Identify and manage challenges,
            security issues, and risks, as well as limitations and opportunities
            in data wrangling.
          </p>

          <p>
            Analysing the GDPR and UK GDPR comparison thread made me look beyond
            regulation into the mindset behind compliance. The peer dialogue
            exposed a tension between simplicity and sufficiency in
            cybersecurity: certification can guide, but also create a false
            sense of safety. Some peers treated GDPR as fixed rules, while
            others saw it as an evolving ethical framework. The discussion about
            “review” as an active verb captured the real spirit of compliance,
            continuous improvement and privacy awareness. I came to view
            compliance not as an obligation but as an architectural principle
            shaping database design, system policies, and workflow culture. It
            reminded me that every technical safeguard involves ethical
            stewardship over data ownership, purpose, and lifespan.
          </p>

          <h2>Artefact - Seminar 5 - Case Study</h2>
          <ul>
            <li>
              <a
                href="modules/deciphering-big-data/artefacts/Seminar 5 - Case Study - Summary.txt"
                target="_blank"
                >Seminar 5 - Case Study - Summary</a
              >
            </li>
            <li>
              <a
                href="modules/deciphering-big-data/artefacts/Seminar 5 - Case Study - PPTX.pptx"
                target="_blank"
                >Seminar Slides (PPTX)</a
              >
            </li>
            <li>
              <a
                href="modules/deciphering-big-data/artefacts/Seminar 5 - Case Study - Transcript.vtt"
                target="_blank"
                >Seminar Transcript (VTT)</a
              >
            </li>
          </ul>

          <h2>References</h2>
          <ul>
            <li>
              European Union (2016)
              <em
                >Regulation (EU) 2016/679 of the European Parliament and of the
                Council of 27 April 2016 on the protection of natural persons
                with regard to the processing of personal data</em
              >. Available at:
              <a
                href="https://eur-lex.europa.eu/eli/reg/2016/679/oj"
                target="_blank"
                >https://eur-lex.europa.eu/eli/reg/2016/679/oj</a
              >.
            </li>
            <li>
              United Kingdom (2018)
              <em>UK General Data Protection Regulation (UK GDPR)</em>.
              Available at:
              <a
                href="https://www.legislation.gov.uk/eur/2016/679/article/32"
                target="_blank"
                >https://www.legislation.gov.uk/eur/2016/679/article/32</a
              >.
            </li>
            <li>
              Information Commissioner’s Office (ICO) (2023)
              <em>A guide to data security</em>. Available at:
              <a
                href="https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/security/"
                target="_blank"
                >https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/security/</a
              >.
            </li>
            <li>
              GDPRAdvisor (n.d.) <em>Cyber Essentials</em>. Available at:
              <a
                href="https://www.gdpradvisor.co.uk/cyber-essentials"
                target="_blank"
                >https://www.gdpradvisor.co.uk/cyber-essentials</a
              >.
            </li>
            <li>
              Williams, G. (2025)
              <em>Seminar 5: Compliance and Data Regulation</em> [Lecture
              material]. University of Essex Online.
            </li>
          </ul>
        </div>
      </section>

      <!-- Nine -->
      <section id="nine" class="wrapper style1 fade-up">
        <div class="inner">
          <h2>Unit 9: Database Management Systems (DBMS) and Models</h2>

          <h2 style="margin-bottom: 5px">Reflection</h2>
          <p>
            <strong>What:</strong> This unit connected data modelling with
            real-world database implementation, covering schema design,
            indexing, transactions, and concurrency control. I learned how
            middleware and scripting tools like Python connectors and ODBC link
            applications to relational systems.
          </p>

          <p>
            <strong>So what:</strong> The seminar showed that databases evolve
            with business needs and that scalability depends on design choices
            such as indexing and normalisation. It aligned with my experience in
            Power Apps and KNIME, where I handle data refresh and schema issues
            that often stem from underlying DBMS design. Recognising this
            improved my confidence in diagnosing and optimising system
            performance.
          </p>

          <p>
            <strong>What next:</strong> I plan to build more structured data
            architectures in Power Apps, integrating SQL Server or PostgreSQL
            for stronger data governance. Linking KNIME workflows to relational
            databases will further automate analytics while ensuring data
            integrity. contexts.
          </p>

          <h2>Artefact - Seminar 6 Reflection</h2>
          <p>
            <strong>Seminar Overview:</strong> This seminar, led by Dr. Godfried
            Williams, explored the design and implementation of relational
            database management systems. It expanded on the Dream Home case
            study, where a company required a property management system to
            handle listings and rentals. The session connected theoretical data
            modeling concepts to practical database deployment using MySQL, SQL
            Server, and SQLite, focusing on data storage, manipulation, and
            accessibility in distributed environments.
          </p>

          <p>
            <strong>Learning Outcomes:</strong> Understand DBMS architecture and
            principles, apply SQL for creating and managing databases, evaluate
            scalability and integrity challenges, and recognise the importance
            of multi-user systems in distributed data environments.
          </p>

          <p>
            The seminar highlighted how resilient databases rely on thoughtful
            architecture. Peer discussions on distributed and hybrid systems,
            including SQLite, challenged assumptions that scalability requires
            large cloud platforms. Reflecting on my automation projects, I saw
            how Power Apps and KNIME mirror DBMS logic, reinforcing that good
            design balances structure, flexibility, and performance.
          </p>

          <ul>
            <li>
              <a
                href="modules/deciphering-big-data/artefacts/Seminar 6 - Building a DBMS - Summary.txt"
                target="_blank"
                >Seminar 6 - Building a DBMS - Summary</a
              >
            </li>
            <li>
              <a
                href="modules/deciphering-big-data/artefacts/Seminar 6 - Building a DBMS - PPTX.pptx"
                target="_blank"
                >Seminar Slides (PPTX)</a
              >
            </li>
            <li>
              <a
                href="modules/deciphering-big-data/artefacts/Seminar 6 - Building a DBMS - Transcript.vtt"
                target="_blank"
                >Seminar Transcript (VTT)</a
              >
            </li>
          </ul>

          <h2>References</h2>
          <ul>
            <li>
              Connolly, T. and Begg, C. (2014)
              <em
                >Database Systems: A Practical Approach to Design,
                Implementation and Management</em
              >. 6th edn. Pearson.
            </li>
            <li>
              McKinney, W. (2022)
              <em
                >Python for Data Analysis: Data Wrangling with Pandas, NumPy,
                and Jupyter</em
              >. 3rd edn. O’Reilly.
            </li>
            <li>
              IBM (2022) <em>Data Management and Middleware Concepts</em>. IBM
              Developer Resources.
            </li>
            <li>
              Williams, G. (2025) <em>Seminar 6: Building a DBMS</em> [Lecture
              material]. University of Essex Online.
            </li>
          </ul>
        </div>
      </section>

      <!-- Ten -->
      <section id="ten" class="wrapper style2 fade-up">
        <div class="inner">
          <h2>
            Unit 10: More on APIs (Application Programming Interfaces) for Data
            Parsing
          </h2>

          <h2 style="margin-bottom: 5px">Reflection</h2>
          <p>
            <strong>What:</strong> This task combined theoretical and applied
            API security design. Analysing IBM’s QRadar DSM API through OWASP
            and NIST frameworks taught me how APIs can serve as both enablers
            and vulnerabilities. I led technical research, drafting
            authentication flows, mutual TLS configurations, and aligning
            documentation with DevSecOps standards.
          </p>

          <p>
            <strong>So what:</strong> The project highlighted that security must
            be embedded throughout the API lifecycle. Many risks, such as
            injection and rate-limit breaches, mirrored issues I had encountered
            in Power Apps and automation flows. Balancing usability with strict
            access control was challenging but essential. Collaborating on this
            report showed how integrating security and usability perspectives
            leads to stronger, more credible outcomes.
          </p>

          <p>
            <strong>What next:</strong> I plan to implement token-based
            authentication and automated security validation in Power Apps and
            KNIME workflows, using tools such as Postman and OWASP ZAP to ensure
            configuration integrity.
          </p>

          <h2>Artefact - API Security Requirements Specification</h2>
          <h3>e-Portfolio Component: API Security Requirements</h3>
          <p>
            <strong>Task Overview:</strong> This unit explored the practical and
            security considerations of designing and deploying APIs. Working
            collaboratively with Sonya Jackson, we developed a comprehensive
            <em>API Security Requirements Specification</em> for the IBM QRadar
            Device Support Module (DSM) API. The task required identifying
            security risks and defining protective controls related to
            authentication, rate limiting, encryption, and compliance. Our paper
            reflected on how robust API design underpins both technical
            resilience and organisational trust.
          </p>

          <p>
            <strong>Learning Outcomes:</strong> Evaluate API security
            mechanisms, apply principles of confidentiality and integrity in
            data transmission, and understand compliance-driven requirements in
            API architecture.
          </p>

          <p>
            Collaborative report defining authentication, encryption,
            validation, and compliance safeguards for IBM’s QRadar DSM API. It
            demonstrated how structured API design enhances confidentiality,
            integrity, and availability, while aligning with GDPR and NIST
            standards.
          </p>

          <a
            href="modules/deciphering-big-data/artefacts/api-task.docx"
            target="_blank"
            >Written document can be downloaded here: API Security Requirements
            Specification
          </a>

          <br /><br />
          <h2>References</h2>
          <ul>
            <li>
              IBM (2025) <em>IBM QRadar SIEM API Documentation</em>. Available
              at:
              <a
                href="https://www.ibm.com/docs/en/qradar-common?topic=api-endpoint-documentation-supported-versions"
                target="_blank"
                >https://www.ibm.com/docs/en/qradar-common?topic=api-endpoint-documentation-supported-versions</a
              >.
            </li>
            <li>
              OWASP (2023) <em>OWASP API Security Top 10 – 2023</em>. Available
              at:
              <a href="https://owasp.org/API-Security" target="_blank"
                >https://owasp.org/API-Security</a
              >.
            </li>
            <li>
              NIST (2023)
              <em
                >NIST SP 800-204C: Implementation of DevSecOps for a
                Microservices-Based Application Using Service Mesh</em
              >. National Institute of Standards and Technology. Available at:
              <a href="https://doi.org/10.6028/NIST.SP.800-204C" target="_blank"
                >https://doi.org/10.6028/NIST.SP.800-204C</a
              >.
            </li>
            <li>
              Williams, G. (2025)
              <em>Unit 10: More on APIs for Data Parsing</em> [Lecture
              material]. University of Essex Online.
            </li>
          </ul>
        </div>
      </section>

      <!-- Eleven -->
      <section id="eleven" class="wrapper style3 fade-up">
        <div class="inner">
          <h2>Unit 11: DBMS Transaction and Recovery</h2>

          <h2 style="margin-bottom: 5px">Reflection</h2>
          <p>
            <strong>What:</strong> This unit reinforced the importance of transaction management and recovery mechanisms in maintaining database reliability. Studying ACID principles and backup strategies clarified why transaction design underpins dependable data systems. The Back-Up Procedure exercise showed how recovery methods prevent loss and maintain consistency.
          </p>

          <p>
            <strong>So what:</strong> Understanding transaction logs and isolation levels helped me diagnose data conflicts I have faced in Power Apps and KNIME. The unit also showed that recovery strategies must balance resource use with downtime, reinforcing the importance of proactive design.
          </p>

          <p>
            <strong>What next:</strong> I will apply transactional safeguards and checkpoint validation in automation workflows, reviewing current backup frequencies and integrating systematic verification routines.
          </p>

          <h2>Artefact – Back-Up Procedure Evaluation</h2>
          <p>
            The task required critically evaluating the
            <em>Grandfather–Father–Son (GFS)</em> backup strategy and its
            effectiveness for large-scale data systems. The GFS model cycles
            through three tiers of backups: daily (sons), weekly (fathers), and
            monthly (grandfathers). This layered retention structure balances
            resource efficiency with recovery reliability.
          </p>

          <p>
            Compared to differential or incremental backups, GFS offers strong
            redundancy without overwhelming storage, as older backups are
            rotated rather than continuously accumulated. For large
            transactional databases, such as those hosted on PostgreSQL or SQL
            Server, this reduces input/output strain while maintaining
            sufficient recovery points. However, the process can still be
            resource-intensive if full backups are too frequent or not properly
            scheduled. Integrating automation tools and cloud-based versioning
            (such as AWS RDS snapshots) can make GFS more adaptive for modern
            environments.
          </p>

          <p>
            In conclusion, GFS remains a viable and efficient method when
            aligned with transaction logging and incremental recovery planning.
            Its simplicity and layered design make it practical for hybrid
            systems that combine on-premises and cloud backups, especially where
            auditability and restoration time are critical.
          </p>

          <h2>Artefact – Individual Project: Executive Summary</h2>
          <p>
            The final individual project summarised the design of a scalable and
            compliant
            <strong>PostgreSQL database system hosted on AWS</strong> to manage
            clinical trial data. The report integrated data normalization, audit
            trails, encryption, and GDPR-compliant architecture. It highlighted
            how relational modeling and cloud-based replication ensure both
            operational reliability and regulatory alignment.
          </p>

          <p>
            I contributed to the normalization design and the integration of
            compliance controls into the logical schema. The project emphasized
            how technical design decisions, such as query optimization and
            storage indexing, intersect with ethical and legal responsibilities
            in handling sensitive health data.
          </p>

          <ul>
            <li>
              <a
                href="modules/deciphering-big-data/artefacts/Individual%20Project%20Executive%20Summary.docx"
                target="_blank"
                >Individual Project – Executive Summary (DOCX)</a
              >
            </li>
          </ul>

          <h2>References</h2>
          <ul>
            <li>
              Amazon Web Services (2025)
              <em>Amazon RDS for PostgreSQL – Features</em>. Available at:
              <a href="https://aws.amazon.com/rds/postgresql/" target="_blank"
                >https://aws.amazon.com/rds/postgresql/</a
              >
            </li>
            <li>
              European Medicines Agency (2023)
              <em
                >Guideline on Computerised Systems and Electronic Data in
                Clinical Trials (EMA/INS/GCP/112288/2023)</em
              >. Available at:
              <a
                href="https://www.ema.europa.eu/en/documents/regulatory-procedural-guideline/guideline-computerised-systems-and-electronic-data-clinical-trials_en.pdf"
                target="_blank"
                >https://www.ema.europa.eu/en/...</a
              >
            </li>
            <li>
              European Union (2016)
              <em
                >Regulation (EU) 2016/679 of the European Parliament and of the
                Council (General Data Protection Regulation)</em
              >. Official Journal of the European Union, L119, 1–88. Available
              at:
              <a
                href="https://eur-lex.europa.eu/eli/reg/2016/679/oj"
                target="_blank"
                >https://eur-lex.europa.eu/eli/reg/2016/679/oj</a
              >
            </li>
            <li>
              Sarkar, T. and Roychowdhury, S. (2019)
              <em
                >Data Wrangling with Python: Creating Actionable Data from Raw
                Sources</em
              >. Birmingham: Packt Publishing Ltd.
            </li>
            <li>
              Weissler, E. H. et al. (2021) ‘The role of machine learning in
              clinical research: transforming the future of evidence
              generation’, <em>Trials</em>, 22(1), p. 537.
            </li>
            <li>
              Williams, G. (2025)
              <em>Unit 11: DBMS Transaction and Recovery</em> [Lecture
              material]. University of Essex Online.
            </li>
          </ul>
        </div>
      </section>

      <!-- Twelve -->
      <section id="twelve" class="wrapper style1 fade-up">
        <div class="inner">
          <h2>Unit 12: Future of Big Data Analytics</h2>

          <h2 style="margin-bottom: 5px">Reflection</h2>
          <p>
            <strong>What:</strong> This final unit consolidated the skills developed throughout the module, linking database design, automation, and analytics. Discussions on machine learning and big data highlighted how predictive systems extend traditional data management and depend on earlier foundations such as normalisation and compliance.
          </p>

          <p>
            <strong>So what:</strong> The “Content Challenge” showed how my understanding has matured. I now see data wrangling, APIs, and analytics as parts of one integrated pipeline where structure determines reliability. Revisiting ACID properties and updateable views clarified how solid database architecture prevents fragility and supports scalability in systems like Power Apps and KNIME.
          </p>

          <p>
            <strong>What next:</strong> I aim to merge automation and AI responsibly by extending KNIME workflows with predictive components for anomaly detection and by adding audit logging and version control to Power Apps projects to improve reliability.
          </p>

          <h2>Artefact – Seminar 7: Content Challenge</h2>
          <p>
            <strong>Seminar Overview:</strong> This concluding seminar, led by
            Dr. Godfried Williams, served as a reflective synthesis of the
            module. It revisited key concepts such as data wrangling, data
            security, risks, and limitations in data handling, encouraging
            critical reflection on the evolution of our technical and analytical
            skills. The “Content Challenge” provided guiding questions on
            database management, ACID transactions, privileges, and views,
            fostering a final discussion that connected technical precision with
            reflective insight.
          </p>

          <p>
            <strong>Learning Outcomes:</strong> Consolidate module knowledge,
            critically evaluate data management approaches, and identify future
            opportunities for professional growth in data science and analytics.
          </p>
          <ul>
            <li>
              <a
                href="modules/deciphering-big-data/artefacts/Seminar%207%20-%20Content%20Challenge%20-%20Summary.txt"
                target="_blank"
                >Seminar 7 – Content Challenge Summary</a
              >
            </li>
            <li>
              <a
                href="modules/deciphering-big-data/artefacts/Seminar%207%20-%20Content%20Challenge%20-%20PPTX.pptx"
                target="_blank"
                >Seminar Slides (PPTX)</a
              >
            </li>
            <li>
              <a
                href="modules/deciphering-big-data/artefacts/Seminar%207%20-%20Content%20Challenge%20-%20Transcript.vtt"
                target="_blank"
                >Seminar Transcript (VTT)</a
              >
            </li>
          </ul>

          <h2>References</h2>
          <ul>
            <li>
              Williams, G. (2025) <em>Seminar 7: Content Challenge</em> [Lecture
              material]. University of Essex Online.
            </li>
            <li>
              Tejada, Z. (2024) <em>Big Data Architectures</em>. O’Reilly Media.
            </li>
            <li>
              McKinney, W. (2022)
              <em
                >Python for Data Analysis: Data Wrangling with Pandas, NumPy,
                and Jupyter</em
              >. 3rd edn. O’Reilly.
            </li>
            <li>
              IBM (2024)
              <em>Future Trends in Data Science and Machine Learning</em>. IBM
              Developer Reports.
            </li>
          </ul>
        </div>
      </section>
    </div>

    <!-- Footer -->
    <footer id="footer" class="wrapper style1-alt">
      <div class="inner">
        <ul class="menu">
          <li>&copy; Untitled. All rights reserved.</li>
          <li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
        </ul>
      </div>
    </footer>

    <!-- Scripts -->
    <script src="assets/js/jquery.min.js"></script>
    <script src="assets/js/jquery.scrollex.min.js"></script>
    <script src="assets/js/jquery.scrolly.min.js"></script>
    <script src="assets/js/browser.min.js"></script>
    <script src="assets/js/breakpoints.min.js"></script>
    <script src="assets/js/util.js"></script>
    <script src="assets/js/main.js"></script>
  </body>
</html>
