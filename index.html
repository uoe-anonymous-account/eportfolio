<!DOCTYPE html>
<!--
	Hyperspace by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
  <head>
    <title>Hyperspace by HTML5 UP</title>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, user-scalable=no"
    />
    <link rel="stylesheet" href="assets/css/main.css" />
    <noscript
      ><link rel="stylesheet" href="assets/css/noscript.css"
    /></noscript>
  </head>
  <body class="is-preload">
    <!-- Sidebar -->
    <section id="sidebar">
      <div class="inner">
        <nav>
          <ul>
            <li><a href="#intro">Welcome</a></li>
            <li><a href="#one">Unit 1</a></li>
            <li><a href="#two">Unit 2</a></li>
            <li><a href="#three">Unit 3</a></li>
            <li><a href="#four">Unit 4</a></li>
            <li><a href="#five">Unit 5</a></li>
            <li><a href="#six">Unit 6</a></li>
            <li><a href="#seven">Unit 7</a></li>
            <li><a href="#eight">Unit 8</a></li>
            <li><a href="#nine">Unit 9</a></li>
            <li><a href="#ten">Unit 10</a></li>
            <li><a href="#eleven">Unit 11</a></li>
            <li><a href="#twelve">Unit 12</a></li>
          </ul>
        </nav>
      </div>
    </section>

    <!-- Wrapper -->
    <div id="wrapper">
      <!-- Intro -->
      <section id="intro" class="wrapper style1 fullscreen fade-up">
        <div class="inner">
          <h1>e-Portfolio</h1>
          <p>Master’s in Data Science | University of Essex Online</p>
          <p>
            Versatile and goal-oriented Software Engineer with distinct data
            science, automation, and front-end development experience. I am most
            productive in high-energy and diverse settings, always seeking to
            develop meaningful and user-centered solutions. I consider myself
            receptive to learning and change by constantly taking on new skills
            and remaining up to date with developments in technology. My
            collective mindset and approach to constructive critique prove
            useful in both group and personal responsibilities.
          </p>
          <a
            href="modules/deciphering-big-data/artefacts/Anonymous CV.pdf"
            target="_blank"
            style="font-size: 50px"
            >Full CV</a
          >
        </div>
      </section>

      <!-- One -->
      <section id="one" class="wrapper style2 fade-up">
        <div class="inner">
          <h2>
            Unit 1: Introduction to Big Data Technologies and Data Management
          </h2>

          <h2 style="margin-bottom: 5px">Reflection</h2>
          <p>
            <strong>What:</strong> During Unit 1, the tutor has taught us how to
            identify and evaluate various data formats (structure,
            semi-structured, and unstructured), and also how to understand the
            impact of data storage and security on system performance.
            <br /><br />
            <strong>So what:</strong> This Unit has aided me in connecting
            theoretical knowledge of big data with practical implementation,
            particularly when it comes to using Python for cleaning and
            transformation processes of data.<br /><br />
            <strong>What next:</strong> In future units, I aim to apply these
            concepts to real-world data sets and strengthen my data validation
            and visualization skills.
          </p>

          <h2>Artefact: Big Data Overview in Swiss Cities</h2>
          <p>
            This Jupyter Notebook explores a simple dataset of Swiss cities,
            comparing population size, data centre presence, and internet
            speeds.
          </p>

          <iframe
            src=""
            width="100%"
            height="800px"
          >
          </iframe>
          <i
            ><p>
              If nbviewer doesn't work, please access the file at the Github:
              <a
                href=""
                target="_blank"
              >
                Big Data - Introduction
              </a>
            </p></i
          >

          <h3>Documents and notes from the Seminar of Unit 1:</h3>
          <ul>
            <li>
              <a
                href="modules/deciphering-big-data/artefacts/Seminar 1  - Introduction - Summary.txt"
                target="_blank"
                >Seminar Summary (TXT)</a
              >
            </li>
            <li>
              <a
                href="modules/deciphering-big-data/artefacts/Seminar 1 - Introduction - PPTX"
                target="_blank"
                >Seminar Slides (PPTX)</a
              >
            </li>
            <li>
              <a
                href="modules/deciphering-big-data/artefacts/Seminar 1 - Introduction - Transcript.vtt"
                target="_blank"
                >Seminar Transcript (VTT)</a
              >
            </li>
          </ul>

          <h2>References</h2>
          <p>
            McKinney, W. (2022)
            <em
              >Python for Data Analysis: Data Wrangling with Pandas, NumPy, and
              Jupyter.</em
            >
            3rd edn. Sebastopol, California: O’Reilly.<br />
            Sardar, T. H. and Pandey, B.K. (2024)
            <em>Big Data Computing.</em> CRC Press.<br />
            Huxley et al. (2020) <em>Data Cleaning.</em> Sage Foundation.
          </p>
        </div>
      </section>

      <!-- Two -->
      <section id="two" class="wrapper style3 fade-up">
        <div class="inner">
          <h2>Unit 2: Data Files and Types</h2>
          <h2 style="margin-bottom: 5px">Reflection</h2>
          <p>
            <strong>What:</strong> This unit explored data files, formats, and
            representations; how file structure determines accessibility and
            storage efficiency. We compared structured, semi-structured, and
            unstructured formats through the EMC (2015) pyramid model and
            practised using Python data types.<br /><br />
            <strong>So what:</strong> Initially, I viewed data formats and IoT
            architecture as purely technical constructs, but engaging with peer
            perspectives made me realise how design decisions embed ethical
            trade-offs. I recognised a personal bias towards performance over
            privacy, a mindset that this discussion challenged. Reflecting on
            the tension between decentralisation and accountability led me to
            reconsider how I approach data pipeline design in my own work
            environment.<br /><br />
            <strong>What next:</strong> Going forward, I aim to test small-scale
            edge deployments using open datasets to examine how preprocessing
            affects latency, cost, and compliance under GDPR. This practical
            application will also allow me to evaluate my own communication and
            collaboration skills by documenting results and seeking peer
            feedback, addressing the participation gap from this unit.
          </p>
          <h2>Artefacts</h2>
          <ul>
            <li>
              <h3>Reflection on IoT Collaborative Discussion</h3>
              <p>
                <strong>What:</strong> I reviewed peer discussions on the role
                of IoT and fog computing in data processing and privacy, notably
                Sonya Jackson’s contribution. Her post analysed the exponential
                data growth challenge and how fog computing mitigates latency
                issues.
              </p>
              <p>
                <strong>So what:</strong> This expanded my understanding of
                decentralised processing and highlighted the ethical dimensions
                of edge AI and data ownership. I realised how similar principles
                apply to real-time data pipelines in my own work. Although
                initially frustrated about missing the group activity, this
                reflection helped me turn that lapse into an opportunity to
                build self-awareness and accountability.
              </p>
              <p>
                <strong>What next:</strong> I plan to research how local
                preprocessing models reduce cloud dependency and enhance
                compliance with GDPR in IoT analytics.
              </p>
              <p>
                As I was unable to participate in the "Collaborative Discussion
                1 - The Data Collection Process", I emailed the tutor and
                requested advice, to which I was encouraged to further reflect
                on observations and teachings. This exchange can be seen below.
              </p>
              <img
                width="110%"
                src="modules/deciphering-big-data/artefacts/email-with-tutor.png"
                alt=""
              />
            </li>
          </ul>

          <h2>References</h2>
          <p>
            Kazil, J. and Jarmul, K. (2016)
            <em>Data Wrangling with Python.</em> O’Reilly Media.<br />
            Services, EMC (2015)
            <em>Data Science and Big Data Analytics.</em> Wiley Professional
            Development.<br />
            Ohlhorst, F. J. (2012)
            <em>Big Data Analytics: Turning Big Data into Big Money.</em> Wiley.
          </p>
        </div>
      </section>

      <!-- Three -->
      <section id="three" class="wrapper style1 fade-up">
        <div class="inner">
          <h2>Unit 3: Data Collection and Storage</h2>

          <h2 style="margin-bottom: 5px">Reflection</h2>

          <p>
            <strong>What:</strong> This unit focused on collecting and
            validating data from online sources while addressing integrity,
            reliability, and format challenges. I explored how web scraping
            supports automated data acquisition and how structured storage
            (JSON, XML) enhances later data wrangling and visualization.
          </p>

          <p>
            <strong>So what:</strong> Building a scraper using
            <em>BeautifulSoup</em> and <em>Requests</em> gave me practical
            insight into how APIs and HTML parsing differ in flexibility and
            complexity. I realised that reliable data collection depends on
            ethical considerations, including respecting website policies and
            managing personal data responsibly under GDPR. Debugging the parser
            also deepened my understanding of HTML hierarchies and response
            handling.
          </p>

          <p>
            <strong>What next:</strong> In future work, I plan to extend this
            project by scraping multiple pages and consolidating results into a
            database. This will allow me to test automated data pipelines for
            larger-scale analytics while refining error handling and compliance
            checks.
          </p>

          <h2>Artefact: Web Scraping Demonstration</h2>
          <p>
            This artefact demonstrates a Python-based scraper that collects
            headings and introductory text from a live Wikipedia article and
            exports the data to both JSON and XML formats.
          </p>

          <iframe
            src=""
            width="100%"
            height="800px"
          >
          </iframe>
          <i
            ><p>
              If nbviewer doesn't work, please access the file at the Github:
              <a
                href=""
                target="_blank"
              >
                Web Scraping
              </a>
            </p></i
          >
          <br />
          <div class="artefact-images">
            <figure>
              <p></p>
              <img
                src="modules/deciphering-big-data/artefacts/unit3_json.png"
                alt="Preview of the generated JSON file"
                width="90%"
              />
              <figcaption>
                Figure 2 – JSON output showing structured scraped data.
              </figcaption>
            </figure>

            <figure>
              <p></p>
              <img
                src="modules/deciphering-big-data/artefacts/unit3_xml.png"
                alt="Browser view of the XML output"
                width="90%"
              />
              <figcaption>
                Figure 3 – XML file displaying article summary and section
                headings.
              </figcaption>
            </figure>
          </div>
          <br />
          <h2>References</h2>
          <ul>
            <li>
              Kazil, J. and Jawal, K. (2016)
              <em>Data Wrangling with Python</em>. O’Reilly Media.
            </li>
            <li>
              Ohlhorst, F. J. (2012)
              <em>Big Data Analytics: Turning Big Data into Big Money</em>.
              Wiley.
            </li>
            <li>
              Beautiful Soup Documentation (2025) Available at:
              <a
                href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/"
                target="_blank"
                >https://www.crummy.com/software/BeautifulSoup/bs4/doc/</a
              >
            </li>
          </ul>
        </div>
      </section>

      <!-- Four-->
      <section id="four" class="wrapper style2 fade-up">
        <div class="inner">
          <h2>Unit 4: Data Cleaning and Transformation</h2>

          <h2 style="margin-bottom: 5px">Reflection</h2>

          <p>
            <strong>What:</strong> This unit introduced data cleaning and
            transformation principles within Python, including pipeline design,
            code efficiency, and automation. I explored how to standardize data
            structures, handle missing values, and apply best coding practices
            following PEP-8 and modular design conventions.
          </p>

          <p>
            <strong>So what:</strong> Completing the quiz reinforced the
            practical importance of writing clean, testable, and reusable code.
            It highlighted how thoughtful syntax, documentation, and version
            control improve not just readability but long-term maintainability
            in real-world data pipelines. I realized that technical proficiency
            alone is not enough — sustainable data management requires
            consistent coding discipline and clarity.
          </p>

          <p>
            <strong>What next:</strong> I plan to further develop my Python
            cleaning workflow using libraries such as <em>pandas</em> and
            <em>NumPy</em>, integrating automated validation checks to improve
            dataset reliability. I will also adopt formal code reviews and unit
            testing practices to ensure accuracy during future projects.
          </p>

          <h2>Artefact: Data Management Pipeline Quiz</h2>
          <p>
            This artefact showcases my perfect score in the formative “Data
            Management Pipeline Test,” which assessed Python cleaning
            techniques, syntax conventions, and design automation principles.
            <br />
            This test felt like a small checkpoint, but it ended up showing how
            much of Python’s logic I’d internalised without realising it.
            Getting a perfect score on the first try wasn’t just luck — it came
            from how often I’ve debugged messy code and learned to spot patterns
            quickly. Concepts like list comprehensions, syntax rules, and best
            practices no longer felt abstract; they just made sense. But scoring
            100% also made me think about what it doesn’t show. Real data work
            is rarely that neat — it’s full of exceptions, unclear
            documentation, and quick fixes. The quiz tested what I know, but not
            how I handle the unexpected. That contrast made me reflect on the
            difference between knowledge and adaptability. What I took from it
            most was awareness — the ability to pause and think about how I
            solve problems, not just whether I can. Going forward, I want to
            keep using small self-tests like this to check not just accuracy,
            but how flexible and thoughtful I am in applying what I’ve learned.
          </p>

          <div class="artefact-images">
            <figure>
              <img
                src="modules/deciphering-big-data/artefacts/unit4-quiz-question1.png"
                alt="Screenshot of quiz question 1 result"
                width="90%"
              />
              <figcaption>
                Figure 1 – Python concept matching demonstrating comprehension
                of syntax and logical flow in cleaning operations.
              </figcaption>
            </figure>

            <br /><br />
            <figure>
              <img
                src="modules/deciphering-big-data/artefacts/unit4-quiz-question2.png"
                alt="Screenshot of quiz question 2 result"
                width="90%"
              />
              <figcaption>
                Figure 2 – Correct mapping of best practices in repository
                structure, documentation, and test design.
              </figcaption>
            </figure>
          </div>

          <br />
          <h2>References</h2>
          <ul>
            <li>
              McKinney, W. (2022)
              <em
                >Python for Data Analysis: Data Wrangling with Pandas, NumPy,
                and Jupyter</em
              >. 3rd edn. O’Reilly Media.
            </li>
            <li>
              Huxley et al. (2020) <em>Data Cleaning</em>. Sage Foundation.
            </li>
            <li>
              PEP 8 – Style Guide for Python Code (2025). Available at:
              <a href="https://peps.python.org/pep-0008/" target="_blank"
                >https://peps.python.org/pep-0008/</a
              >
            </li>
          </ul>
        </div>
      </section>

      <!-- Five-->
      <section id="five" class="wrapper style3 fade-up">
        <div class="inner">
          <h2>Unit 5: Data Cleaning, Automation and Validation</h2>

          <h2 style="margin-bottom: 5px">Reflection</h2>

          <p>
            <strong>What:</strong> This unit explored how automation and
            validation ensure reliability in data processing workflows. Through
            Seminar 3 on <em>Data Investigations</em>, I examined how errors and
            biases can persist even in automated systems if data sources are not
            verified or maintained properly. The discussion linked directly to
            my web scraping work from Unit 3, showing how critical it is to log
            every step of data collection and transformation for transparency.
          </p>

          <p>
            <strong>So what:</strong> This made me reflect on how easily
            automation can create a false sense of confidence. I realised that
            even technically correct scripts can produce misleading outcomes
            when the input data lacks quality or context. My previous web
            scraping exercise, for example, could be expanded by adding
            validation steps — checking whether headings or content still exist
            before processing. The session also reinforced that data cleaning is
            both a technical and ethical task: efficiency should never come at
            the cost of accountability.
          </p>

          <p>
            <strong>What next:</strong> I plan to build automated workflows that
            include verification points, ensuring each output is backed by
            reproducible logic. For instance, I want to integrate logging
            functions into my Python scripts to record which sources were
            accessed, when, and how. This approach would strengthen the
            reliability of my future analyses and align with professional data
            governance practices.
          </p>

          <h2>Artefact: Seminar 3 – Data Investigations</h2>

          <ul>
            <p style="margin-bottom: 5px">
              This artefact represents my engagement with Seminar 3, which
              examined data quality, automation, and ethical responsibility in
              modern data workflows. The materials below illustrate my analysis
              and participation in that session.
            </p>
            <li>
              <a
                href="modules/deciphering-big-data/artefacts/Seminar%203%20-%20Data%20Investigations%20-%20Summary.txt"
                target="_blank"
                >Seminar Summary (TXT)</a
              >
            </li>
            <li>
              <a
                href="modules/deciphering-big-data/artefacts/Seminar 3 - Data Investigations - PPTX.pptx"
                target="_blank"
                >Seminar Slides (PPTX)</a
              >
            </li>
            <li>
              <a
                href="modules/deciphering-big-data/artefacts/Seminar%203%20-%20Data%20Investigations%20-%20Transcript.vtt"
                target="_blank"
                >Seminar Transcript (VTT)</a
              >
            </li>
          </ul>

          <h2>References</h2>
          <ul>
            <li>
              Raschka, S. (2021)
              <em>Machine Learning and Data Cleaning in Practice</em>. Packt
              Publishing.
            </li>
            <li>
              Kazil, J. and Jarmul, K. (2016)
              <em>Data Wrangling with Python</em>. O’Reilly Media.
            </li>
            <li>
              University of Essex Online (2025) Seminar 3:
              <em>Data Investigations</em> [Lecture material].
            </li>
          </ul>
        </div>
      </section>

      <!-- Six-->
      <section id="six" class="wrapper style1 fade-up">
        <div class="inner">
          <h2>Unit 6: Group Project – Clinical Research Database Design</h2>

          <h2 style="margin-bottom: 5px">Reflection</h2>
          <p>
            <strong>What:</strong> This unit focused on collaborative project
            work and applied database design. I joined the group late and was
            initially presented with an incomplete draft that lacked critical
            depth and clear structure. The project required developing a secure
            and compliant data management pipeline for clinical research,
            aligning with GDPR and Good Clinical Practice (GCP) principles. My
            main contributions included rewriting the report for coherence,
            strengthening the critical analysis, and producing the core visuals
            such as the Entity Relationship Diagram (ERD) and Data Management
            Pipeline model.
          </p>

          <p>
            <strong>So what:</strong> Taking ownership of an existing group
            draft forced me to assess both content quality and collaboration
            dynamics. While my team had produced a functional outline, it lacked
            the conceptual clarity and evaluative insight expected at this
            stage. Integrating technical, ethical, and regulatory perspectives
            into a unified document was challenging, especially while
            coordinating asynchronously with peers who had already divided their
            roles. However, revising the report helped me realise how essential
            critical synthesis and structured visualisation are for
            demonstrating understanding in collaborative projects. The contrast
            between the initial draft and the final submission reflected the
            analytical rigour and design thinking I brought to the group.
          </p>

          <p>
            <strong>What next:</strong> This experience showed me the importance
            of adaptability in group projects and the value of constructive
            leadership through contribution rather than authority. In future
            collaborations, I intend to engage earlier and define
            responsibilities clearly to maintain balance between shared
            ownership and quality control. I also plan to build on this work by
            refining the ERD and pipeline into a prototype system using
            PostgreSQL and automated validation routines.
          </p>

          <h2>Artefacts</h2>
          <ul>
            <li>
              <a
                href="modules/deciphering-big-data/artefacts/Data%20Management%20Pipeline%20and%20Database%20Design%20for%20Clinical%20Research%20Compliance.docx"
                target="_blank"
                >Final Report – Data Management Pipeline and Database Design for
                Clinical Research Compliance</a
              >
            </li>
            <li>
              <a
                href="modules/deciphering-big-data/artefacts/Initial%20group%20draft.docx"
                target="_blank"
                >Initial Group Draft</a
              >
            </li>
            <li>
              <a
                href="modules/deciphering-big-data/artefacts/Peer%20Evaluation.pdf"
                target="_blank"
                >Peer Evaluation</a
              >
            </li>
          </ul>

          <h2>References</h2>
          <ul>
            <li>
              European Union (2016)
              <em>General Data Protection Regulation (GDPR)</em>.
            </li>
            <li>
              Sarkar, T. & Roychowdhury, S. (2019)
              <em>Data Wrangling with Python.</em> Packt Publishing.
            </li>
            <li>
              European Medicines Agency (2023)
              <em
                >Guideline on Computerised Systems and Electronic Data in
                Clinical Trials.</em
              >
            </li>
            <li>
              Williams, G. (2025) <em>Deciphering Big Data</em> [Module
              materials]. University of Essex Online.
            </li>
          </ul>
        </div>
      </section>

      <!-- Seven-->
      <section id="seven" class="wrapper style2 fade-up">
        <div class="inner">
          <h2>Unit 7: Normalisation and Relational Database Construction</h2>

          <h2 style="margin-bottom: 5px">Reflection</h2>
          <p>
            <strong>What:</strong> This unit applied relational database
            principles in practice through a normalisation exercise and the SQL
            Normalisation seminar. Using a denormalised dataset, I decomposed
            the table step-by-step into 1NF, 2NF, and 3NF, creating relational
            tables with primary and foreign keys. The seminar by Dr. Williams
            reinforced these concepts by explaining how structured data storage
            prevents anomalies and improves data integrity across business
            systems.
          </p>

          <p>
            <strong>So what:</strong> Completing the task made me appreciate how
            easily redundancy can distort datasets and how normalisation ensures
            consistency and reliability. Initially, I saw normalisation as a
            mechanical task, but the exercise clarified its strategic role in
            scalability and analytics. It also helped me understand why
            organisations sometimes partially denormalise data for performance —
            a trade-off I had not previously considered. Linking this with SQL
            commands such as <code>CREATE TABLE</code> and
            <code>JOIN</code> deepened my confidence in building efficient data
            models that are both functional and logically structured.
          </p>

          <p>
            <strong>What next:</strong> I plan to apply these relational
            modelling techniques when working with structured sources in Python
            and PostgreSQL, ensuring data pipelines remain normalised until
            transformation or aggregation stages. This experience also
            encouraged me to use visual schema diagrams early in the design
            process to detect dependencies before implementation.
          </p>

          <h2>Artefact - Data Normalisation Task</h2>
          <iframe
            src=""
            width="100%"
            height="800"
            frameborder="0"
            loading="lazy"
          >
          </iframe>
          <i
            ><p>
              If nbviewer doesn't work, please access the file at the Github:
              <a
                href=""
                target="_blank"
              >
                Data Normalisation
              </a>
            </p></i
          >
          <div class="downloads">
            <h3>View normalized tables output:</h3>
            <ul>
              <li>
                <a
                  href=""
                  target="_blank"
                  >Courses</a
                >
              </li>
              <li>
                <a
                  href=""
                  target="_blank"
                  >Enrollments</a
                >
              </li>
              <li>
                <a
                  href=""
                  target="_blank"
                  >Students</a
                >
              </li>
              <li>
                <a
                  href=""
                  target="_blank"
                  >Teachers</a
                >
              </li>
            </ul>
          </div>

          <h2>Artefact - Seminar 4 - SQL Normalisation</h2>
          <ul>
            <li>
              <a
                href=""
                target="_blank"
                >Seminar 4 – SQL Normalisation Summary</a
              >
            </li>
            <li>
              <a
                href="modules/deciphering-big-data/artefacts/Seminar%204%20-%20SQL%20Normalization%20-%20PPTX.pptx"
                target="_blank"
                >Seminar Slides (PPTX)</a
              >
            </li>
            <li>
              <a
                href="modules/deciphering-big-data/artefacts/Seminar%204%20-%20SQL%20Normalization%20-%20Transcript.vtt"
                target="_blank"
                >Seminar Transcript (VTT)</a
              >
            </li>
          </ul>

          <h2>References</h2>
          <ul>
            <li>
              Kazil, J. and Jarmul, K. (2016)
              <em>Data Wrangling with Python</em>. O’Reilly Media.
            </li>
            <li>
              Williams, G. (2025)
              <em>Seminar 4: SQL and Normalisation</em> [Lecture material].
              University of Essex Online.
            </li>
            <li>
              Codd, E. F. (1970) ‘A relational model of data for large shared
              data banks’, <em>Communications of the ACM</em>, 13(6), pp.
              377–387.
            </li>
          </ul>
        </div>
      </section>

      <!-- Eigth -->
      <section id="eight" class="wrapper style3 fade-up">
        <div class="inner">
          <h2>Unit 8: Compliance and Regulatory Framework for Managing Data</h2>

          <h2 style="margin-bottom: 5px">Reflection</h2>
          <p>
            <strong>What:</strong> This unit examined compliance frameworks such
            as the EU GDPR, UK GDPR, and other regulatory standards shaping data
            protection practices. The collaborative discussion focused on the
            principle of data security and compared how these regulations are
            implemented and enforced across regions. Engaging with peer
            discussions gave me a clearer view of the shared ethical backbone
            behind these frameworks and the subtle legal distinctions introduced
            post-Brexit.
          </p>

          <p>
            <strong>So what:</strong> Reading through peers’ interpretations of
            compliance revealed how differently organisations approach
            regulation depending on sector and jurisdiction. Many emphasised
            certification schemes like <em>Cyber Essentials</em> or ISO 27001 as
            practical anchors for compliance. Others pointed to the cultural gap
            between policy and implementation, where compliance is seen as a
            checklist rather than a living framework. This made me reflect on
            how easily compliance fatigue can undermine data protection,
            particularly when technical safeguards become detached from user
            awareness or management accountability.
          </p>

          <p>
            <strong>What next:</strong> I intend to bring a more integrated
            perspective to compliance in my own work by aligning system design
            with both legal and ethical principles. Rather than treating
            frameworks like GDPR as constraints, I see them as enablers of trust
            and transparency. I plan to continue developing my understanding of
            data governance by examining how automated pipelines can maintain
            audit trails and consent mechanisms from collection to processing.
          </p>

          <h2>Artefact - Collaborative Discussion 2 Reflection</h2>
          <p>
            <strong>Discussion Topic:</strong> Compare the rules of the GDPR, in
            particular with relation to the securing of personal data rule, with
            either similar compliance laws within your country of residence or
            with the ICO in the UK. The ICO refers to this rule as
            <em>Security</em> and asks organisations to ensure that personal
            data is processed in a manner that guarantees appropriate security
            and prevents unauthorised access (<a
              href="https://ico.org.uk/"
              target="_blank"
              >ICO.org.uk</a
            >).
          </p>
          <p>
            <strong>Learning Outcomes:</strong> Identify and manage challenges,
            security issues, and risks, as well as limitations and opportunities
            in data wrangling.
          </p>

          <p>
            Analysing the GDPR and UK GDPR comparison thread pushed me to think
            beyond the surface of compliance and into the mindset behind it.
            What stood out most in the peer dialogue was the tension between
            simplicity and sufficiency in cybersecurity practices. While many
            peers argued that compliance provides a standardised path to
            security, I began to see how overreliance on formal certification
            can create a false sense of safety.
          </p>

          <p>
            I noticed that some posts treated GDPR as a static rulebook, while
            others recognised it as a dynamic framework rooted in ethical
            responsibility. This divergence made me realise that compliance
            effectiveness depends less on the law itself and more on how
            organisations internalise it. In one post, the notion of "review"
            within the UK GDPR was discussed as an active verb, not a checkbox.
            That struck me as the true spirit of compliance: the constant
            re-evaluation of security practices, the cultural embedding of
            privacy awareness, and the recognition that protecting personal data
            is never fully complete.
          </p>

          <p>
            From a professional lens, this discussion shifted my perspective
            from compliance as obligation to compliance as architecture,
            something that can shape system design decisions, database
            structures, and workflow policies. The human dimension of this
            conversation reminded me that behind every technical protocol lies
            the ethical question of stewardship: whose data it is, why it’s
            being processed, and how long it should exist. This understanding
            will continue to influence how I document, validate, and secure
            datasets in future data projects.
          </p>

          <h2>Artefact - Seminar 5 - Case Study</h2>
          <ul>
            <li>
              <a
                href="modules/deciphering-big-data/artefacts/Seminar 5 - Case Study - Summary.txt"
                target="_blank"
                >Seminar 5 - Case Study - Summary</a
              >
            </li>
            <li>
              <a
                href="modules/deciphering-big-data/artefacts/Seminar 5 - Case Study - PPTX.pptx"
                target="_blank"
                >Seminar Slides (PPTX)</a
              >
            </li>
            <li>
              <a
                href="modules/deciphering-big-data/artefacts/Seminar 5 - Case Study - Transcript.vtt"
                target="_blank"
                >Seminar Transcript (VTT)</a
              >
            </li>
          </ul>

          <h2>References</h2>
          <ul>
            <li>
              European Union (2016)
              <em
                >Regulation (EU) 2016/679 of the European Parliament and of the
                Council of 27 April 2016 on the protection of natural persons
                with regard to the processing of personal data</em
              >. Available at:
              <a
                href="https://eur-lex.europa.eu/eli/reg/2016/679/oj"
                target="_blank"
                >https://eur-lex.europa.eu/eli/reg/2016/679/oj</a
              >.
            </li>
            <li>
              United Kingdom (2018)
              <em>UK General Data Protection Regulation (UK GDPR)</em>.
              Available at:
              <a
                href="https://www.legislation.gov.uk/eur/2016/679/article/32"
                target="_blank"
                >https://www.legislation.gov.uk/eur/2016/679/article/32</a
              >.
            </li>
            <li>
              Information Commissioner’s Office (ICO) (2023)
              <em>A guide to data security</em>. Available at:
              <a
                href="https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/security/"
                target="_blank"
                >https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/security/</a
              >.
            </li>
            <li>
              GDPRAdvisor (n.d.) <em>Cyber Essentials</em>. Available at:
              <a
                href="https://www.gdpradvisor.co.uk/cyber-essentials"
                target="_blank"
                >https://www.gdpradvisor.co.uk/cyber-essentials</a
              >.
            </li>
            <li>
              Williams, G. (2025)
              <em>Seminar 5: Compliance and Data Regulation</em> [Lecture
              material]. University of Essex Online.
            </li>
          </ul>
        </div>
      </section>

      <!-- Nine -->
      <section id="nine" class="wrapper style1 fade-up">
        <div class="inner">
          <h2>Unit 9: Database Management Systems (DBMS) and Models</h2>

          <h2 style="margin-bottom: 5px">Reflection</h2>
          <p>
            <strong>What:</strong> This unit bridged the gap between data
            modeling and database implementation, showing how relational
            principles translate into real business systems. The seminar
            introduced key DBMS components including schema design, indexing,
            transaction handling, and concurrency control. I also learned how
            middleware and scripting tools like Python connectors or ODBC
            interfaces bridge the application layer and the database layer.
          </p>

          <p>
            <strong>So what:</strong> This seminar helped me understand that
            databases are not isolated systems but living infrastructures that
            evolve alongside business needs. The Dream Home example demonstrated
            how system design decisions, such as normalisation depth or
            indexing, affect long-term scalability and reliability. It also
            aligned closely with my professional experience in Power Apps and
            KNIME automation, where I frequently deal with data flows between
            relational tables and application interfaces. I recognised that many
            of the challenges I have encountered, such as slow data refresh or
            schema mismatch, are fundamentally DBMS design issues rather than
            software limitations. Understanding these principles made me more
            confident in diagnosing and optimising data systems.
          </p>

          <p>
            <strong>What next:</strong> I plan to continue applying these
            concepts by designing more structured data architectures within
            Power Apps and connecting them to external databases like SQL Server
            or PostgreSQL for better data management. I also intend to integrate
            KNIME workflows with relational databases to automate recurring
            analytics tasks while maintaining data integrity. By combining
            automation and database literacy, I can create more transparent and
            maintainable data pipelines in both academic and professional
            contexts.
          </p>

          <h2>Artefact - Seminar 6 Reflection</h2>
          <p>
            <strong>Seminar Overview:</strong> This seminar, led by Dr. Godfried
            Williams, explored the design and implementation of relational
            database management systems. It expanded on the Dream Home case
            study, where a company required a property management system to
            handle listings and rentals. The session connected theoretical data
            modeling concepts to practical database deployment using MySQL, SQL
            Server, and SQLite, focusing on data storage, manipulation, and
            accessibility in distributed environments.
          </p>

          <p>
            <strong>Learning Outcomes:</strong> Understand DBMS architecture and
            principles, apply SQL for creating and managing databases, evaluate
            scalability and integrity challenges, and recognise the importance
            of multi-user systems in distributed data environments.
          </p>

          <p>
            This seminar encouraged me to think critically about what makes a
            database system resilient and adaptable. Listening to peers discuss
            distributed DBMS models made me realise how much of system
            reliability depends on the architecture chosen at the design stage.
            I found the discussion on SQLite particularly eye-opening because it
            challenged the assumption that effective data systems must always be
            large-scale or cloud-hosted. In fact, lightweight and embedded
            databases can often provide faster and simpler solutions when
            designed thoughtfully.
          </p>

          <p>
            I also reflected on how my experience with Power Apps relates
            directly to DBMS principles. Every component, from tables and forms
            to Power Automate flows, essentially represents a simplified
            interface for a relational backend. Recognising this parallel made
            me appreciate the underlying SQL logic and how structured schema
            design prevents the kind of redundancy and data inconsistency I’ve
            seen in poorly built apps. Similarly, my work in KNIME taught me how
            visual workflows can complement relational structures by handling
            validation and automation in repeatable ways. Seeing these
            technologies through a DBMS lens helped me connect academic theory
            to professional reality.
          </p>

          <p>
            The seminar also underscored how data systems increasingly rely on
            hybrid architectures that combine relational and non-relational
            models. I am now more aware that the future of database design lies
            not in choosing one paradigm but in balancing structure with
            flexibility. This awareness will guide me as I design automation
            workflows and integrated data solutions where consistency,
            traceability, and performance coexist.
          </p>

          <ul>
            <li>
              <a
                href="modules/deciphering-big-data/artefacts/Seminar 6 - Building a DBMS - Summary.txt"
                target="_blank"
                >Seminar 6 - Building a DBMS - Summary</a
              >
            </li>
            <li>
              <a
                href="modules/deciphering-big-data/artefacts/Seminar 6 - Building a DBMS - PPTX.pptx"
                target="_blank"
                >Seminar Slides (PPTX)</a
              >
            </li>
            <li>
              <a
                href="modules/deciphering-big-data/artefacts/Seminar 6 - Building a DBMS - Transcript.vtt"
                target="_blank"
                >Seminar Transcript (VTT)</a
              >
            </li>
          </ul>

          <h2>References</h2>
          <ul>
            <li>
              Connolly, T. and Begg, C. (2014)
              <em
                >Database Systems: A Practical Approach to Design,
                Implementation and Management</em
              >. 6th edn. Pearson.
            </li>
            <li>
              McKinney, W. (2022)
              <em
                >Python for Data Analysis: Data Wrangling with Pandas, NumPy,
                and Jupyter</em
              >. 3rd edn. O’Reilly.
            </li>
            <li>
              IBM (2022) <em>Data Management and Middleware Concepts</em>. IBM
              Developer Resources.
            </li>
            <li>
              Williams, G. (2025) <em>Seminar 6: Building a DBMS</em> [Lecture
              material]. University of Essex Online.
            </li>
          </ul>
        </div>
      </section>

      <!-- Ten -->
      <section id="ten" class="wrapper style2 fade-up">
        <div class="inner">
          <h2>
            Unit 10: More on APIs (Application Programming Interfaces) for Data
            Parsing
          </h2>

          <h2 style="margin-bottom: 5px">Reflection</h2>
          <p>
            <strong>What:</strong> This task combined theory and applied
            security design. By analysing IBM’s QRadar DSM API and referencing
            standards such as OWASP and NIST, I learned how APIs can become both
            enablers and vulnerabilities in interconnected data systems. My main
            contribution focused on defining authentication flows, specifying
            mutual TLS configurations, and structuring the document to align
            with DevSecOps standards. While Sonya contributed to final editing
            and formatting, I took the lead on the technical research and
            requirement drafting.
          </p>

          <p>
            <strong>So what:</strong> Completing this task revealed how security
            must be integrated throughout the API lifecycle rather than added at
            the end. Many of the risks we analysed, such as SQL injection and
            excessive request rates, reminded me of challenges I had already
            encountered in Power Apps and automation scripts, where unsecured
            endpoints can expose sensitive data. I also realised how difficult
            it is to balance usability with strict access control. The writing
            process required us to think like both developers and security
            auditors, evaluating not only the functionality but also the attack
            surface of each endpoint.
          </p>

          <p>
            <strong>What next:</strong> I plan to apply these lessons by
            introducing structured security validation in my own automation
            workflows. For example, implementing token-based authentication when
            integrating Power Apps or KNIME flows with APIs will help protect
            against misuse and maintain data privacy. I also intend to explore
            advanced security testing tools such as Postman and OWASP ZAP to
            verify API configurations more rigorously. This experience
            strengthened my appreciation for collaborative work, as combining
            technical depth with peer feedback resulted in a more complete and
            credible deliverable.
          </p>

          <h2>Artefact - API Security Requirements Specification</h2>
          <h3>e-Portfolio Component: API Security Requirements</h3>
          <p>
            <strong>Task Overview:</strong> This unit explored the practical and
            security considerations of designing and deploying APIs. Working
            collaboratively with Sonya Jackson, we developed a comprehensive
            <em>API Security Requirements Specification</em> for the IBM QRadar
            Device Support Module (DSM) API. The task required identifying
            security risks and defining protective controls related to
            authentication, rate limiting, encryption, and compliance. Our paper
            reflected on how robust API design underpins both technical
            resilience and organisational trust.
          </p>

          <p>
            <strong>Learning Outcomes:</strong> Evaluate API security
            mechanisms, apply principles of confidentiality and integrity in
            data transmission, and understand compliance-driven requirements in
            API architecture.
          </p>

          <p>
            The joint deliverable outlined comprehensive technical safeguards
            for securing IBM’s QRadar DSM API. It addressed authentication,
            encryption, data validation, and monitoring controls designed to
            ensure confidentiality, integrity, and availability. The project
            emphasised risk mitigation practices such as enforcing HTTPS,
            adopting OAuth 2.0, validating payloads against schemas, and
            implementing rate limits to prevent denial-of-service attacks. We
            also incorporated compliance measures consistent with GDPR and NIST
            recommendations. The artefact ultimately demonstrated how
            structured, documented security design can reduce complexity and
            operational risk in API ecosystems.
          </p>

          <a
            href="modules/deciphering-big-data/artefacts/api-task.docx"
            target="_blank"
            >Written document can be downloaded here: API Security Requirements
            Specification
          </a>

          <br /><br />
          <h2>References</h2>
          <ul>
            <li>
              IBM (2025) <em>IBM QRadar SIEM API Documentation</em>. Available
              at:
              <a
                href="https://www.ibm.com/docs/en/qradar-common?topic=api-endpoint-documentation-supported-versions"
                target="_blank"
                >https://www.ibm.com/docs/en/qradar-common?topic=api-endpoint-documentation-supported-versions</a
              >.
            </li>
            <li>
              OWASP (2023) <em>OWASP API Security Top 10 – 2023</em>. Available
              at:
              <a href="https://owasp.org/API-Security" target="_blank"
                >https://owasp.org/API-Security</a
              >.
            </li>
            <li>
              NIST (2023)
              <em
                >NIST SP 800-204C: Implementation of DevSecOps for a
                Microservices-Based Application Using Service Mesh</em
              >. National Institute of Standards and Technology. Available at:
              <a href="https://doi.org/10.6028/NIST.SP.800-204C" target="_blank"
                >https://doi.org/10.6028/NIST.SP.800-204C</a
              >.
            </li>
            <li>
              Williams, G. (2025)
              <em>Unit 10: More on APIs for Data Parsing</em> [Lecture
              material]. University of Essex Online.
            </li>
          </ul>
        </div>
      </section>

      <!-- Eleven -->
      <section id="eleven" class="wrapper style3 fade-up">
        <div class="inner">
          <h2>Unit 11: DBMS Transaction and Recovery</h2>

          <h3>Overview</h3>
          <p>
            This unit explored the principles of transaction management, ACID
            properties, and database recovery. It examined how systems maintain
            data integrity during system failures, concurrent transactions, and
            scheduled backups. The focus was on achieving a consistent database
            state, ensuring that transactions either fully commit or fully roll
            back, thus preserving reliability and consistency in
            high-availability systems.
          </p>

          <h2 style="margin-bottom: 5px">Reflection</h2>
          <p>
            <strong>What:</strong> This unit reinforced the importance of
            transaction management and recovery mechanisms in maintaining
            database reliability. Through the readings and the Back-Up Procedure
            activity, I learned how different recovery strategies prevent data
            loss during failures. The discussion of ACID principles—atomicity,
            consistency, isolation, and durability—provided a structured way to
            understand why transaction design lies at the core of every
            dependable data system.
          </p>

          <p>
            <strong>So what:</strong> The exploration of transaction logs,
            checkpoints, and failure recovery methods deepened my appreciation
            for the complexity behind systems I use daily. In my own Power Apps
            and KNIME projects, I had often noticed how concurrent updates
            sometimes caused data conflicts. Understanding transaction isolation
            levels clarified why such issues occur and how mechanisms like
            rollback and write-ahead logging preserve integrity. It also
            highlighted how backup strategies differ depending on workload and
            architecture, with trade-offs between recovery time and resource
            consumption.
          </p>

          <p>
            <strong>What next:</strong> I intend to apply these concepts by
            designing transactional safeguards in my automation workflows. For
            example, when building multi-step data updates in KNIME or Power
            Automate, I can implement validation points and checkpoints that
            emulate commit and rollback behavior. Learning about recovery
            scheduling also motivated me to review how often critical data in my
            current systems is backed up and to integrate more systematic
            verification routines.
          </p>

          <h2>Artefact – Back-Up Procedure Evaluation</h2>
          <p>
            The task required critically evaluating the
            <em>Grandfather–Father–Son (GFS)</em> backup strategy and its
            effectiveness for large-scale data systems. The GFS model cycles
            through three tiers of backups: daily (sons), weekly (fathers), and
            monthly (grandfathers). This layered retention structure balances
            resource efficiency with recovery reliability.
          </p>

          <p>
            Compared to differential or incremental backups, GFS offers strong
            redundancy without overwhelming storage, as older backups are
            rotated rather than continuously accumulated. For large
            transactional databases, such as those hosted on PostgreSQL or SQL
            Server, this reduces input/output strain while maintaining
            sufficient recovery points. However, the process can still be
            resource-intensive if full backups are too frequent or not properly
            scheduled. Integrating automation tools and cloud-based versioning
            (such as AWS RDS snapshots) can make GFS more adaptive for modern
            environments.
          </p>

          <p>
            In conclusion, GFS remains a viable and efficient method when
            aligned with transaction logging and incremental recovery planning.
            Its simplicity and layered design make it practical for hybrid
            systems that combine on-premises and cloud backups, especially where
            auditability and restoration time are critical.
          </p>

          <h2>Artefact – Individual Project: Executive Summary</h2>
          <p>
            The final individual project summarised the design of a scalable and
            compliant
            <strong>PostgreSQL database system hosted on AWS</strong> to manage
            clinical trial data. The report integrated data normalization, audit
            trails, encryption, and GDPR-compliant architecture. It highlighted
            how relational modeling and cloud-based replication ensure both
            operational reliability and regulatory alignment.
          </p>

          <p>
            I contributed to the normalization design and the integration of
            compliance controls into the logical schema. The project emphasized
            how technical design decisions, such as query optimization and
            storage indexing, intersect with ethical and legal responsibilities
            in handling sensitive health data.
          </p>

          <ul>
            <li>
              <a
                href="modules/deciphering-big-data/artefacts/Individual%20Project%20Executive%20Summary.docx"
                target="_blank"
                >Individual Project – Executive Summary (DOCX)</a
              >
            </li>
          </ul>

          <h2>References</h2>
          <ul>
            <li>
              Amazon Web Services (2025)
              <em>Amazon RDS for PostgreSQL – Features</em>. Available at:
              <a href="https://aws.amazon.com/rds/postgresql/" target="_blank"
                >https://aws.amazon.com/rds/postgresql/</a
              >
            </li>
            <li>
              European Medicines Agency (2023)
              <em
                >Guideline on Computerised Systems and Electronic Data in
                Clinical Trials (EMA/INS/GCP/112288/2023)</em
              >. Available at:
              <a
                href="https://www.ema.europa.eu/en/documents/regulatory-procedural-guideline/guideline-computerised-systems-and-electronic-data-clinical-trials_en.pdf"
                target="_blank"
                >https://www.ema.europa.eu/en/...</a
              >
            </li>
            <li>
              European Union (2016)
              <em
                >Regulation (EU) 2016/679 of the European Parliament and of the
                Council (General Data Protection Regulation)</em
              >. Official Journal of the European Union, L119, 1–88. Available
              at:
              <a
                href="https://eur-lex.europa.eu/eli/reg/2016/679/oj"
                target="_blank"
                >https://eur-lex.europa.eu/eli/reg/2016/679/oj</a
              >
            </li>
            <li>
              Sarkar, T. and Roychowdhury, S. (2019)
              <em
                >Data Wrangling with Python: Creating Actionable Data from Raw
                Sources</em
              >. Birmingham: Packt Publishing Ltd.
            </li>
            <li>
              Weissler, E. H. et al. (2021) ‘The role of machine learning in
              clinical research: transforming the future of evidence
              generation’, <em>Trials</em>, 22(1), p. 537.
            </li>
            <li>
              Williams, G. (2025)
              <em>Unit 11: DBMS Transaction and Recovery</em> [Lecture
              material]. University of Essex Online.
            </li>
          </ul>
        </div>
      </section>

      <!-- Twelve -->
      <section id="twelve" class="wrapper style1 fade-up">
        <div class="inner">
          <h2>Unit 12: Future of Big Data Analytics</h2>

          <h2 style="margin-bottom: 5px">Reflection</h2>
          <p>
            <strong>What:</strong> This final unit acted as a checkpoint for
            integrating all the skills and theoretical foundations developed
            across the module. The lecture and seminar discussions on machine
            learning and big data analytics broadened my understanding of how
            predictive systems extend traditional data management. Revisiting
            earlier topics, from normalisation to compliance, helped me
            recognise how these building blocks interconnect to support scalable
            and secure analytical systems.
          </p>

          <p>
            <strong>So what:</strong> Reflecting on the “Content Challenge”
            highlighted how much my perspective has evolved. At the beginning of
            the course, I often saw data wrangling and automation as separate
            layers of work. Now, I view them as part of a unified pipeline where
            database structure, API design, and analytics models all influence
            each other. The questions about ACID properties and updateable views
            were particularly meaningful, as they reinforced how data
            reliability principles translate directly into the practical tools I
            use in Power Apps and KNIME. Understanding these links made me
            realise that automation without database awareness leads to
            fragility, while good architecture ensures both resilience and
            adaptability.
          </p>

          <p>
            <strong>What next:</strong> I intend to continue exploring how
            automation and AI can merge within structured data environments. My
            immediate goal is to extend my KNIME workflows with predictive
            components, applying basic machine learning to detect anomalies or
            optimise task scheduling. I also want to adopt more advanced
            practices from this module, such as audit logging and version
            control, in my Power Apps projects to make them more robust. This
            final reflection reinforced my belief that the next step in my
            professional development lies in bridging automation, data
            engineering, and AI responsibly and transparently.
          </p>

          <h2>Artefact – Seminar 7: Content Challenge</h2>
          <p>
            <strong>Seminar Overview:</strong> This concluding seminar, led by
            Dr. Godfried Williams, served as a reflective synthesis of the
            module. It revisited key concepts such as data wrangling, data
            security, risks, and limitations in data handling, encouraging
            critical reflection on the evolution of our technical and analytical
            skills. The “Content Challenge” provided guiding questions on
            database management, ACID transactions, privileges, and views,
            fostering a final discussion that connected technical precision with
            reflective insight.
          </p>

          <p>
            <strong>Learning Outcomes:</strong> Consolidate module knowledge,
            critically evaluate data management approaches, and identify future
            opportunities for professional growth in data science and analytics.
          </p>
          <ul>
            <li>
              <a
                href="modules/deciphering-big-data/artefacts/Seminar%207%20-%20Content%20Challenge%20-%20Summary.txt"
                target="_blank"
                >Seminar 7 – Content Challenge Summary</a
              >
            </li>
            <li>
              <a
                href="modules/deciphering-big-data/artefacts/Seminar%207%20-%20Content%20Challenge%20-%20PPTX.pptx"
                target="_blank"
                >Seminar Slides (PPTX)</a
              >
            </li>
            <li>
              <a
                href="modules/deciphering-big-data/artefacts/Seminar%207%20-%20Content%20Challenge%20-%20Transcript.vtt"
                target="_blank"
                >Seminar Transcript (VTT)</a
              >
            </li>
          </ul>

          <h2>References</h2>
          <ul>
            <li>
              Williams, G. (2025) <em>Seminar 7: Content Challenge</em> [Lecture
              material]. University of Essex Online.
            </li>
            <li>
              Tejada, Z. (2024) <em>Big Data Architectures</em>. O’Reilly Media.
            </li>
            <li>
              McKinney, W. (2022)
              <em
                >Python for Data Analysis: Data Wrangling with Pandas, NumPy,
                and Jupyter</em
              >. 3rd edn. O’Reilly.
            </li>
            <li>
              IBM (2024)
              <em>Future Trends in Data Science and Machine Learning</em>. IBM
              Developer Reports.
            </li>
          </ul>
        </div>
      </section>

      <section id="module-reflection" class="wrapper style2 fade-up">
        <div class="inner">
          <h2>Overall Module Reflection: Deciphering Big Data</h2>

          <p>
            Looking back over the twelve units of this module, I can clearly see
            how my understanding of data has evolved from technical application
            to strategic thinking. At the start, I approached topics such as
            data wrangling, automation, and compliance as separate components.
            By the end, I began to recognise them as interdependent layers
            within the broader data lifecycle. The module helped me move beyond
            procedural knowledge into a mindset that values structure,
            accountability, and ethical design in every stage of data
            processing.
          </p>

          <p>
            One of the most transformative aspects of this learning experience
            was connecting academic theory with real-world systems I manage at
            Thermo Fisher. Concepts such as normalisation, ACID transactions,
            and API security became more than coursework exercises. They
            directly related to how I approach Power Apps development, Dataverse
            architecture, and KNIME automation in practice. I began to see how
            issues I had encountered in daily work, such as data redundancy or
            system lag, could often be traced back to weak relational structure
            or unverified automation logic. Understanding these principles has
            allowed me to create cleaner, more scalable, and more compliant
            systems at work.
          </p>

          <p>
            Collaboration also played a significant role in my growth. Working
            with peers like Sonya Jackson on the API security task and engaging
            in compliance discussions gave me new perspectives on teamwork,
            documentation, and shared accountability. These interactions
            reminded me that technical excellence means little without
            communication and reflection. I became more confident in
            articulating design choices, defending data models, and balancing
            performance against compliance requirements.
          </p>

          <p>
            The most valuable takeaway from this module is the awareness that
            good data practice is not just about efficiency, but about
            stewardship. Whether designing a database, automating workflows, or
            analysing sensitive data, every decision has ethical and operational
            implications. This awareness will guide me as I continue to refine
            the Lengnau Power Apps ecosystem and pursue further study in machine
            learning and advanced analytics. The module has strengthened both my
            technical and reflective capabilities, giving me the confidence to
            design systems that are not only intelligent and automated but also
            transparent, secure, and compliant.
          </p>

          <h2>References</h2>
          <ul>
            <li>
              Williams, G. (2025) <em>Deciphering Big Data</em> [Module
              material]. University of Essex Online.
            </li>
            <li>
              European Union (2016)
              <em>General Data Protection Regulation (GDPR)</em>. Official
              Journal of the European Union, L119, 1–88.
            </li>
            <li>
              Connolly, T. and Begg, C. (2014)
              <em
                >Database Systems: A Practical Approach to Design,
                Implementation and Management</em
              >. 6th edn. Pearson.
            </li>
            <li>
              McKinney, W. (2022)
              <em
                >Python for Data Analysis: Data Wrangling with Pandas, NumPy,
                and Jupyter</em
              >. 3rd edn. O’Reilly.
            </li>
            <li>
              O’Neil, C. and Schutt, R. (2013) <em>Doing Data Science</em>.
              O’Reilly Media.
            </li>
          </ul>
        </div>
      </section>

    </div>

    <!-- Footer -->
    <footer id="footer" class="wrapper style1-alt">
      <div class="inner">
        <ul class="menu">
          <li>&copy; Untitled. All rights reserved.</li>
          <li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
        </ul>
      </div>
    </footer>

    <!-- Scripts -->
    <script src="assets/js/jquery.min.js"></script>
    <script src="assets/js/jquery.scrollex.min.js"></script>
    <script src="assets/js/jquery.scrolly.min.js"></script>
    <script src="assets/js/browser.min.js"></script>
    <script src="assets/js/breakpoints.min.js"></script>
    <script src="assets/js/util.js"></script>
    <script src="assets/js/main.js"></script>
  </body>
</html>
